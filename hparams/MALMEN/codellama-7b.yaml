alg_name: "MALMEN"
model_name: ../ptms/CodeLlama-7b-hf
model_class: LlamaForCausalLM
tokenizer_class: CodeLlamaTokenizer
tokenizer_name: ../ptms/CodeLlama-7b-hf
device: 0
# Model
inner_params:
- model.layers.26.mlp.down_proj
- model.layers.27.mlp.down_proj
- model.layers.28.mlp.down_proj
- model.layers.29.mlp.down_proj
- model.layers.30.mlp.down_proj
- model.layers.31.mlp.down_proj


# archive: ./results/models_EditConala/MALMEN/CodeLlama-7b-hf
archive: null
# archive: ./results/models_EditCodeSearchNet/MALMEN/CodeLlama-7b-hf
# Method
alg: MALMEN
dropout: 0.0
train_base: False
no_grad_layers: null

rank: 1920
n_blocks: 2
lr: 1e-6
meta_lr: 1e-5
loc_coef: 1
max_grad_norm: 1
token: ans

# Train
n_edits: 1
batch_size: 4
editor_batch_size: 1024
silent: False
# max_epochs: 1
max_iters: 1000
log_interval: 100
eval_log_interval: 100
final_eval: True
val_interval: 100
early_stop_patience: 100
early_stop_key: "ES_val"
eval_only: False
debug: False
save: False

val_batch_size: 1
val_steps: 200 # only for debug

model_parallel: false

# Output
results_dir: ./results
